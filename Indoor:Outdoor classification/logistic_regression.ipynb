{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Методы Оптимизации\n",
    "## Задача классификации изображений\n",
    "### Студент: Ярослав Спирин (группа 594)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # vectorized math\n",
    "import scipy.misc # image loading\n",
    "import os # to get all images from the directory\n",
    "from tqdm import tqdm_notebook as tqdm # tracking loop progress\n",
    "import matplotlib.pyplot as plt # to show plots and images\n",
    "import matplotlib\n",
    "from scipy.ndimage import uniform_filter\n",
    "from matplotlib.mlab import PCA # PCA encoder\n",
    "import pandas as pd # working with dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку будем использовать генератор случайных чисел, нужно фиксировать seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Для начала загрузим данные и представим каждую картинку в виде вектора признаков следующим образом:\n",
    "![](https://cdn-images-1.medium.com/max/1600/1*Vo8PMHppg_lWxFAZHZzNGQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Поскольку картинки имеют разрешение 32x32 и каждый пиксель отвечает за наполнение 3-мя цветами, у нас получится, что каждая картинка - это элемент пространства размерности 3072. В дальнейшем мы будем использовать метод уменьшения размерности PCA, поскольку данные из пространства размерности 3072, описывающие пиксели картинки - избыточны.\n",
    "\n",
    "** Update: ** Таким было бы решение, если бы не было необходимости занимать верхние места в Kaggle Leaderboard (https://www.kaggle.com/c/fivt-metopt-2017/leaderboard). Чтобы достичь лучших результатов, я предлагаю подход использующий не пиксели в качестве фичей, а более информативные свойства, которыми обладает каждая картинка из датасета. Например, цветовые гистограммы, гистограмма ориентированных градиентов и другие.\n",
    "\n",
    "Некоторые подходы заимствованы из курса cs231n Stanford University."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Возвращает цветовые гистограммы\n",
    "def color_histogram_hsv(im, nbin=10, xmin=0, xmax=255, normalized=True):\n",
    "    ndim = im.ndim\n",
    "    bins = np.linspace(xmin, xmax, nbin+1)\n",
    "    hsv = matplotlib.colors.rgb_to_hsv(im/xmax) * xmax\n",
    "    imhist, bin_edges = np.histogram(hsv[:,:,0], bins=bins, density=normalized)\n",
    "    imhist = imhist * np.diff(bin_edges)\n",
    "\n",
    "    # return histogram\n",
    "    return imhist\n",
    "\n",
    "# Перевести картинку из формата RGB в черно-белый\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.144])\n",
    "\n",
    "# Извлечь гистограмму ориентрованных градиентов\n",
    "# https://en.wikipedia.org/wiki/Histogram_of_oriented_gradients\n",
    "def hog_feature(im):\n",
    "    \n",
    "    # convert rgb to grayscale if needed\n",
    "    if im.ndim == 3:\n",
    "        image = rgb2gray(im)\n",
    "    else:\n",
    "        image = np.at_least_2d(im)\n",
    "\n",
    "    sx, sy = image.shape # image size\n",
    "    orientations = 9 # number of gradient bins\n",
    "    cx, cy = (8, 8) # pixels per cell\n",
    "\n",
    "    gx = np.zeros(image.shape)\n",
    "    gy = np.zeros(image.shape)\n",
    "    gx[:, :-1] = np.diff(image, n=1, axis=1) # compute gradient on x-direction\n",
    "    gy[:-1, :] = np.diff(image, n=1, axis=0) # compute gradient on y-direction\n",
    "    grad_mag = np.sqrt(gx ** 2 + gy ** 2) # gradient magnitude\n",
    "    grad_ori = np.arctan2(gy, (gx + 1e-15)) * (180 / np.pi) + 90 # gradient orientation\n",
    "\n",
    "    n_cellsx = int(np.floor(sx / cx))  # number of cells in x\n",
    "    n_cellsy = int(np.floor(sy / cy))  # number of cells in y\n",
    "    # compute orientations integral images\n",
    "    orientation_histogram = np.zeros((n_cellsx, n_cellsy, orientations))\n",
    "    for i in range(orientations):\n",
    "        # create new integral image for this orientation\n",
    "        # isolate orientations in this range\n",
    "        temp_ori = np.where(grad_ori < 180 / orientations * (i + 1),\n",
    "                        grad_ori, 0)\n",
    "        temp_ori = np.where(grad_ori >= 180 / orientations * i,\n",
    "                        temp_ori, 0)\n",
    "        # select magnitudes for those orientations\n",
    "        cond2 = temp_ori > 0\n",
    "        temp_mag = np.where(cond2, grad_mag, 0)\n",
    "        orientation_histogram[:,:,i] = uniform_filter(temp_mag, size=(cx, cy))[int(cx/2)::cx, int(cy/2)::cy].T\n",
    "  \n",
    "    return orientation_histogram.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Функция извлечения фичей\n",
    "def extract_features(sample):\n",
    "    # hsv\n",
    "    hsv = color_histogram_hsv(sample, nbin=80)\n",
    "    \n",
    "    # hog\n",
    "    hog = hog_feature(sample)\n",
    "    \n",
    "    # color hist\n",
    "    sample = sample.reshape(sample.shape[0] * sample.shape[1], -1)\n",
    "    \n",
    "    red_buckets, _ = np.histogram(sample[:, 0], bins=range(0, 256, 3))\n",
    "    green_buckets, _ = np.histogram(sample[:, 1], bins=range(0, 256, 3))\n",
    "    blue_buckets, _ = np.histogram(sample[:, 2], bins=range(0, 256, 3))\n",
    "    \n",
    "    # volume color hist\n",
    "    volume_color_hist, _ = np.histogramdd(sample, bins=[8, 8, 8])\n",
    "    volume_color_buckets = volume_color_hist.flatten()\n",
    "    \n",
    "    features = np.concatenate((red_buckets, green_buckets, blue_buckets, volume_color_buckets, hog, hsv))\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Функция загружающая картинки из соотвествующих директорий\n",
    "def load_images():\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    flatten_train_images = []\n",
    "    \n",
    "    for filename in tqdm(os.listdir(\"train\"), desc=\"train loop\"):\n",
    "        train_image = scipy.misc.imread(\"train/{}\".format(filename), mode=\"RGB\")\n",
    "        flipped_train_image = np.flip(train_image, axis=1)\n",
    "        \n",
    "        X_train.append(extract_features(train_image))\n",
    "        flatten_train_images.append(train_image.flatten())\n",
    "        \n",
    "        X_train.append(extract_features(flipped_train_image))\n",
    "        flatten_train_images.append(flipped_train_image.flatten())\n",
    "        \n",
    "        if filename.startswith(\"indoor\"):\n",
    "            y_train.append(1)\n",
    "            y_train.append(1)\n",
    "        else:\n",
    "            y_train.append(0)\n",
    "            y_train.append(0)\n",
    "    \n",
    "    X_test = [] \n",
    "    flatten_test_images = []\n",
    "    \n",
    "    for filename in tqdm(os.listdir(\"test\"), desc=\"test loop\"):\n",
    "        test_image = scipy.misc.imread(\"test/{}\".format(filename), mode=\"RGB\")\n",
    "        \n",
    "        X_test.append(extract_features(test_image))\n",
    "        flatten_test_images.append(test_image.flatten())\n",
    "        \n",
    "    X_train = np.array(X_train)\n",
    "    X_test = np.array(X_test)\n",
    "    y_train = np.array(y_train)\n",
    "    \n",
    "    # adding pixels features\n",
    "    flatten_train_images = np.array(flatten_train_images)\n",
    "    flatten_test_images = np.array(flatten_test_images)\n",
    "\n",
    "    num_train = flatten_train_images.shape[0]\n",
    "    num_test = flatten_test_images.shape[0]\n",
    "    \n",
    "    pca_matrix = PCA(np.vstack((flatten_train_images, flatten_test_images)))\n",
    "    pca_matrix = pca_matrix.Y[:, :200]\n",
    "    \n",
    "    img_feature_train = pca_matrix[:num_train]\n",
    "    img_feature_test = pca_matrix[num_train:]\n",
    "    \n",
    "    X_train = np.hstack((X_train, img_feature_train))\n",
    "    X_test = np.hstack((X_test, img_feature_test))\n",
    "    \n",
    "    return X_train, y_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6513bd88d37f477ab66e66fcc0648d29"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f62b581da51d4b0183af0c8271695718"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X, y, X_test = load_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Далее отнормируем фичи по формуле: $$ X = \\frac {X}  {X_{max}} $$, как показала практика такая нормировка оказалась даже лучше общепринятого нормирования $$ X = \\frac {X - X_{min}} {X_{max} - X_{min}}$$ и $$ X = \\frac {X - X_{mean}} {std(X)} $$ После такого преобразования градиентный спуск будет сходится быстрее. (https://en.wikipedia.org/wiki/Feature_scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def normalize_batch(X):\n",
    "    return X / X.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def normalize(X, partitions):\n",
    "    aux = np.hstack((np.array([0]), partitions))\n",
    "    cumsum = np.cumsum(aux)\n",
    "    for i in range(1, len(cumsum)):\n",
    "        X[:, cumsum[i - 1]:cumsum[i]] = normalize_batch(X[:, cumsum[i - 1]:cumsum[i]])\n",
    "        \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "color_bucket_size = len(range(0, 256, 3))\n",
    "volume_buckets_size = 8 * 8 * 8\n",
    "hog_size = 144\n",
    "hsv_size = 80\n",
    "pixels_size = 200\n",
    "\n",
    "partitions = [color_bucket_size,\n",
    "              color_bucket_size,\n",
    "              color_bucket_size,\n",
    "              volume_buckets_size,\n",
    "              hog_size,\n",
    "              hsv_size,\n",
    "              pixels_size\n",
    "             ]\n",
    "\n",
    "num_train = X.shape[0]\n",
    "\n",
    "normalized_data = normalize(np.vstack((X, X_test)), partitions)\n",
    "\n",
    "X = normalized_data[:num_train]\n",
    "X_test = normalized_data[num_train:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Разделим выборку `X` на `X_train` и `X_val` для того, чтобы проверять точность модели. При этом разделение будем производить случайным образом, чтобы исключить дальнейшее возникновение переобучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train_and_val(X, y, ratio=0.75):\n",
    "    # samples with odd indices are flipped images - can't use them in validation\n",
    "    \n",
    "    # 1. split into odd and even rows\n",
    "    X_even = X[::2]\n",
    "    X_odd = X[1::2]\n",
    "    \n",
    "    y_even = y[::2]\n",
    "    y_odd = y[1::2]\n",
    "    \n",
    "    # 2. subsampling from even rows\n",
    "    indices = np.arange(len(X_even))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    num_train = int(ratio * len(X_even))\n",
    "    \n",
    "    X_train = X_even[indices[:num_train]]\n",
    "    X_val = X_even[indices[num_train:]]\n",
    "    \n",
    "    y_train = y_even[indices[:num_train]]\n",
    "    y_val = y_even[indices[num_train:]]\n",
    "    \n",
    "    # 3. subsampling from odd rows\n",
    "    indices = np.arange(len(X_odd))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    X_train = np.vstack((X_train, X_odd[indices]))\n",
    "    y_train = np.hstack((y_train, y_odd[indices]))\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val = train_and_val(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.astype(\"float64\")\n",
    "X_val = X_val.astype(\"float64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изобразим на гистограмме как выглядит один вектор признаков для картинки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nySEFfxGRHFLwFxHJIQV/EZEc+v/2vEtWBkAKdwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1106b4630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(np.arange(X_train[1300].shape[0]), X_train[1300])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Поскольку логистическая регрессия использует линейную модель $y = XW + b$, добавим дополнительный столбец (соответствующий $b$) к `X_train` и `X_test`. Таким образом, мы сделаем работу нашего алгоритма более прямолинейной. Как итог получим модель, которая выглядит более приятно: $y = XW$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Теперь реализуем класс линейного классификатора, основываясь на лекции курса cs231n (Stanford University).\n",
    "\n",
    "Реализация использует аналитически посчитанный градиент для большей точности.\n",
    "\n",
    "$$ scores = XW $$ и $$ softmax = \\frac { e^{scores_{correctclass}} } {\\sum{e^{scores_{class}}}} $$\n",
    "\n",
    "Функцию потерь определим так: $$ Loss = \\frac {1} {N} \\sum{Loss_i} + \\lambda \\sum\\sum{W_{i,j}^2} $$, где $$ Loss_i = -\\log(\\frac { e^{scores_{correctclass}} } {\\sum{e^{scores_{class}}}}) $$\n",
    "\n",
    "Кроме того, классификатор позволяет считать функцию потерь на валидационной выборке и позволяет настраивать шаг градиентного спуска `learning_rate` и регуляризацию `reg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class SoftmaxClassifier:\n",
    "\n",
    "    ### Initialize classifier\n",
    "    def __init__(self):\n",
    "        self.W = None\n",
    "\n",
    "    ### Train the logistic regression model using Gradient Descent algorithm\n",
    "    def train(self, X, y, X_val=None, y_val=None, learning_rate=1e-3, reg=1e-5, num_iters=100, verbose=False):\n",
    "        num_train, dim = X.shape\n",
    "        num_classes = np.max(y) + 1\n",
    "        \n",
    "        if self.W is None:\n",
    "            self.W = 0.0001 * np.random.randn(dim, num_classes)\n",
    "\n",
    "        # Run gradient descent to optimize W\n",
    "        loss_history = []\n",
    "        loss_val_history = []\n",
    "        for it in range(num_iters):\n",
    "            # evaluate loss and gradient\n",
    "            loss, grad, loss_val = self.softmax_loss_vectorized(X, y, reg, X_val, y_val)\n",
    "            \n",
    "            #loss_history.append(loss)\n",
    "            #loss_val_history.append(loss_val)\n",
    "            \n",
    "            # update parameters\n",
    "            self.W += -learning_rate * grad\n",
    "\n",
    "            if verbose and it % 100 == 0:\n",
    "                \n",
    "                if len(loss_val_history) > 0 and loss_val_history[-1] < loss_val:\n",
    "                    break\n",
    "                \n",
    "                loss_history.append(loss)\n",
    "                loss_val_history.append(loss_val)\n",
    "                \n",
    "                print('iteration %d / %d: loss %f; validation loss %f' % (it, num_iters, loss, loss_val))\n",
    "                \n",
    "\n",
    "        return loss_history\n",
    "    \n",
    "    ### Predict the output value for the test sample\n",
    "    def predict(self, X):\n",
    "        scores = np.dot(X, self.W)\n",
    "        max_scores = np.max(scores, axis=1).reshape(scores.shape[0], -1)\n",
    "        scores -= max_scores\n",
    "        probs = np.exp(scores) / np.sum(np.exp(scores), axis=1).reshape(scores.shape[0], -1)\n",
    "        return probs[:, 1]\n",
    "        \n",
    "    ### Calculate the loss and gradient\n",
    "    def softmax_loss_vectorized(self, X, y, reg, X_val, y_val):\n",
    "  \n",
    "        # Initialize the loss and gradient to zero\n",
    "        loss = 0.0\n",
    "        loss_val = 0.0\n",
    "        dW = np.zeros_like(self.W)\n",
    "  \n",
    "        num_train = X.shape[0]\n",
    "        num_classes = self.W.shape[1]\n",
    "\n",
    "        # Loss calculation\n",
    "        scores = np.dot(X, self.W)\n",
    "        max_scores = np.max(scores, axis=1).reshape(scores.shape[0], -1)\n",
    "        scores -= max_scores\n",
    "        \n",
    "        arange = np.arange(0, num_train)\n",
    "    \n",
    "        correct_class_scores = scores[arange, y]\n",
    "        logistic_function = np.exp(correct_class_scores) / np.sum(np.exp(scores), axis=1)\n",
    "        loss += -np.sum(np.log(logistic_function))\n",
    "    \n",
    "        loss /= num_train\n",
    "        loss += reg * np.sum(self.W * self.W)\n",
    "   \n",
    "\n",
    "        # Gradient calculation\n",
    "        aux = np.exp(scores) / np.sum(np.exp(scores), axis=1).reshape(scores.shape[0], -1)\n",
    "        dW += np.dot(X.T, aux)\n",
    "    \n",
    "        correct_classes = np.zeros(scores.shape)\n",
    "        correct_classes[arange, y] = 1\n",
    "        dW -= np.dot(X.T, correct_classes)\n",
    "    \n",
    "        dW /= num_train\n",
    "        dW += 2 * reg * self.W\n",
    "        \n",
    "        # Validation loss calculation\n",
    "        num_train = X_val.shape[0]\n",
    "        \n",
    "        scores = np.dot(X_val, self.W)\n",
    "        max_scores = np.max(scores, axis=1).reshape(scores.shape[0], -1)\n",
    "        scores -= max_scores\n",
    "        \n",
    "        arange = np.arange(0, num_train)\n",
    "    \n",
    "        correct_class_scores = scores[arange, y_val]\n",
    "        logistic_function = np.exp(correct_class_scores) / np.sum(np.exp(scores), axis=1)\n",
    "        loss_val += -np.sum(np.log(logistic_function))\n",
    "    \n",
    "        loss_val /= num_train\n",
    "        loss_val += reg * np.sum(self.W * self.W)\n",
    "        \n",
    "        return loss, dW, loss_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Bootstrapping\n",
    "Данный подход подразумевает то, что мы настраиваем несколько моделей на случайных подвыборках исходного датасета, а затем усредним показатели на тестовой выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax Classifier 1\n",
      "iteration 0 / 3000: loss 0.693109; validation loss 0.693115\n",
      "iteration 100 / 3000: loss 0.177259; validation loss 0.184047\n",
      "iteration 200 / 3000: loss 0.162791; validation loss 0.173344\n",
      "iteration 300 / 3000: loss 0.154399; validation loss 0.167811\n",
      "iteration 400 / 3000: loss 0.148668; validation loss 0.164381\n",
      "iteration 500 / 3000: loss 0.144407; validation loss 0.162076\n",
      "iteration 600 / 3000: loss 0.141058; validation loss 0.160447\n",
      "iteration 700 / 3000: loss 0.138322; validation loss 0.159256\n",
      "iteration 800 / 3000: loss 0.136021; validation loss 0.158364\n",
      "iteration 900 / 3000: loss 0.134044; validation loss 0.157683\n",
      "iteration 1000 / 3000: loss 0.132317; validation loss 0.157158\n",
      "iteration 1100 / 3000: loss 0.130788; validation loss 0.156750\n",
      "iteration 1200 / 3000: loss 0.129419; validation loss 0.156431\n",
      "iteration 1300 / 3000: loss 0.128182; validation loss 0.156181\n",
      "iteration 1400 / 3000: loss 0.127058; validation loss 0.155986\n",
      "iteration 1500 / 3000: loss 0.126028; validation loss 0.155836\n",
      "iteration 1600 / 3000: loss 0.125079; validation loss 0.155721\n",
      "iteration 1700 / 3000: loss 0.124201; validation loss 0.155636\n",
      "iteration 1800 / 3000: loss 0.123386; validation loss 0.155574\n",
      "iteration 1900 / 3000: loss 0.122624; validation loss 0.155532\n",
      "iteration 2000 / 3000: loss 0.121912; validation loss 0.155505\n",
      "iteration 2100 / 3000: loss 0.121242; validation loss 0.155493\n",
      "iteration 2200 / 3000: loss 0.120611; validation loss 0.155491\n",
      "\n",
      "Softmax Classifier 2\n",
      "iteration 0 / 3000: loss 0.693151; validation loss 0.693149\n",
      "iteration 100 / 3000: loss 0.177241; validation loss 0.187098\n",
      "iteration 200 / 3000: loss 0.163297; validation loss 0.173762\n",
      "iteration 300 / 3000: loss 0.155131; validation loss 0.166069\n",
      "iteration 400 / 3000: loss 0.149514; validation loss 0.161096\n",
      "iteration 500 / 3000: loss 0.145320; validation loss 0.157650\n",
      "iteration 600 / 3000: loss 0.142017; validation loss 0.155137\n",
      "iteration 700 / 3000: loss 0.139314; validation loss 0.153235\n",
      "iteration 800 / 3000: loss 0.137040; validation loss 0.151752\n",
      "iteration 900 / 3000: loss 0.135084; validation loss 0.150571\n",
      "iteration 1000 / 3000: loss 0.133373; validation loss 0.149613\n",
      "iteration 1100 / 3000: loss 0.131856; validation loss 0.148828\n",
      "iteration 1200 / 3000: loss 0.130495; validation loss 0.148179\n",
      "iteration 1300 / 3000: loss 0.129264; validation loss 0.147637\n",
      "iteration 1400 / 3000: loss 0.128141; validation loss 0.147184\n",
      "iteration 1500 / 3000: loss 0.127110; validation loss 0.146804\n",
      "iteration 1600 / 3000: loss 0.126159; validation loss 0.146484\n",
      "iteration 1700 / 3000: loss 0.125277; validation loss 0.146215\n",
      "iteration 1800 / 3000: loss 0.124456; validation loss 0.145990\n",
      "iteration 1900 / 3000: loss 0.123687; validation loss 0.145801\n",
      "iteration 2000 / 3000: loss 0.122966; validation loss 0.145644\n",
      "iteration 2100 / 3000: loss 0.122287; validation loss 0.145514\n",
      "iteration 2200 / 3000: loss 0.121645; validation loss 0.145408\n",
      "iteration 2300 / 3000: loss 0.121038; validation loss 0.145322\n",
      "iteration 2400 / 3000: loss 0.120462; validation loss 0.145254\n",
      "iteration 2500 / 3000: loss 0.119914; validation loss 0.145201\n",
      "iteration 2600 / 3000: loss 0.119391; validation loss 0.145161\n",
      "iteration 2700 / 3000: loss 0.118892; validation loss 0.145134\n",
      "iteration 2800 / 3000: loss 0.118414; validation loss 0.145117\n",
      "iteration 2900 / 3000: loss 0.117956; validation loss 0.145109\n",
      "\n",
      "Softmax Classifier 3\n",
      "iteration 0 / 3000: loss 0.693151; validation loss 0.693151\n",
      "iteration 100 / 3000: loss 0.179125; validation loss 0.173989\n",
      "iteration 200 / 3000: loss 0.165133; validation loss 0.160866\n",
      "iteration 300 / 3000: loss 0.157036; validation loss 0.153530\n",
      "iteration 400 / 3000: loss 0.151516; validation loss 0.148655\n",
      "iteration 500 / 3000: loss 0.147418; validation loss 0.145161\n",
      "iteration 600 / 3000: loss 0.144204; validation loss 0.142524\n",
      "iteration 700 / 3000: loss 0.141584; validation loss 0.140454\n",
      "iteration 800 / 3000: loss 0.139385; validation loss 0.138778\n",
      "iteration 900 / 3000: loss 0.137499; validation loss 0.137384\n",
      "iteration 1000 / 3000: loss 0.135854; validation loss 0.136202\n",
      "iteration 1100 / 3000: loss 0.134397; validation loss 0.135180\n",
      "iteration 1200 / 3000: loss 0.133094; validation loss 0.134286\n",
      "iteration 1300 / 3000: loss 0.131917; validation loss 0.133494\n",
      "iteration 1400 / 3000: loss 0.130845; validation loss 0.132785\n",
      "iteration 1500 / 3000: loss 0.129863; validation loss 0.132145\n",
      "iteration 1600 / 3000: loss 0.128957; validation loss 0.131564\n",
      "iteration 1700 / 3000: loss 0.128117; validation loss 0.131034\n",
      "iteration 1800 / 3000: loss 0.127335; validation loss 0.130547\n",
      "iteration 1900 / 3000: loss 0.126603; validation loss 0.130099\n",
      "iteration 2000 / 3000: loss 0.125917; validation loss 0.129685\n",
      "iteration 2100 / 3000: loss 0.125270; validation loss 0.129300\n",
      "iteration 2200 / 3000: loss 0.124659; validation loss 0.128942\n",
      "iteration 2300 / 3000: loss 0.124081; validation loss 0.128609\n",
      "iteration 2400 / 3000: loss 0.123531; validation loss 0.128298\n",
      "iteration 2500 / 3000: loss 0.123008; validation loss 0.128006\n",
      "iteration 2600 / 3000: loss 0.122509; validation loss 0.127733\n",
      "iteration 2700 / 3000: loss 0.122032; validation loss 0.127476\n",
      "iteration 2800 / 3000: loss 0.121575; validation loss 0.127235\n",
      "iteration 2900 / 3000: loss 0.121137; validation loss 0.127007\n",
      "\n",
      "Softmax Classifier 4\n",
      "iteration 0 / 3000: loss 0.693173; validation loss 0.693166\n",
      "iteration 100 / 3000: loss 0.178714; validation loss 0.174803\n",
      "iteration 200 / 3000: loss 0.164708; validation loss 0.161853\n",
      "iteration 300 / 3000: loss 0.156578; validation loss 0.154642\n",
      "iteration 400 / 3000: loss 0.151014; validation loss 0.149962\n",
      "iteration 500 / 3000: loss 0.146870; validation loss 0.146667\n",
      "iteration 600 / 3000: loss 0.143610; validation loss 0.144228\n",
      "iteration 700 / 3000: loss 0.140945; validation loss 0.142359\n",
      "iteration 800 / 3000: loss 0.138703; validation loss 0.140892\n",
      "iteration 900 / 3000: loss 0.136776; validation loss 0.139718\n",
      "iteration 1000 / 3000: loss 0.135090; validation loss 0.138764\n",
      "iteration 1100 / 3000: loss 0.133597; validation loss 0.137979\n",
      "iteration 1200 / 3000: loss 0.132258; validation loss 0.137326\n",
      "iteration 1300 / 3000: loss 0.131047; validation loss 0.136780\n",
      "iteration 1400 / 3000: loss 0.129944; validation loss 0.136319\n",
      "iteration 1500 / 3000: loss 0.128933; validation loss 0.135928\n",
      "iteration 1600 / 3000: loss 0.128000; validation loss 0.135594\n",
      "iteration 1700 / 3000: loss 0.127135; validation loss 0.135308\n",
      "iteration 1800 / 3000: loss 0.126331; validation loss 0.135063\n",
      "iteration 1900 / 3000: loss 0.125579; validation loss 0.134851\n",
      "iteration 2000 / 3000: loss 0.124874; validation loss 0.134668\n",
      "iteration 2100 / 3000: loss 0.124210; validation loss 0.134509\n",
      "iteration 2200 / 3000: loss 0.123584; validation loss 0.134371\n",
      "iteration 2300 / 3000: loss 0.122992; validation loss 0.134251\n",
      "iteration 2400 / 3000: loss 0.122431; validation loss 0.134146\n",
      "iteration 2500 / 3000: loss 0.121897; validation loss 0.134054\n",
      "iteration 2600 / 3000: loss 0.121388; validation loss 0.133974\n",
      "iteration 2700 / 3000: loss 0.120903; validation loss 0.133903\n",
      "iteration 2800 / 3000: loss 0.120439; validation loss 0.133841\n",
      "iteration 2900 / 3000: loss 0.119994; validation loss 0.133787\n",
      "\n",
      "Softmax Classifier 5\n",
      "iteration 0 / 3000: loss 0.693140; validation loss 0.693136\n",
      "iteration 100 / 3000: loss 0.178447; validation loss 0.173579\n",
      "iteration 200 / 3000: loss 0.164164; validation loss 0.163054\n",
      "iteration 300 / 3000: loss 0.155889; validation loss 0.157028\n",
      "iteration 400 / 3000: loss 0.150224; validation loss 0.153101\n",
      "iteration 500 / 3000: loss 0.146001; validation loss 0.150353\n",
      "iteration 600 / 3000: loss 0.142676; validation loss 0.148338\n",
      "iteration 700 / 3000: loss 0.139958; validation loss 0.146808\n",
      "iteration 800 / 3000: loss 0.137671; validation loss 0.145612\n",
      "iteration 900 / 3000: loss 0.135706; validation loss 0.144657\n",
      "iteration 1000 / 3000: loss 0.133989; validation loss 0.143879\n",
      "iteration 1100 / 3000: loss 0.132468; validation loss 0.143237\n",
      "iteration 1200 / 3000: loss 0.131106; validation loss 0.142700\n",
      "iteration 1300 / 3000: loss 0.129875; validation loss 0.142247\n",
      "iteration 1400 / 3000: loss 0.128753; validation loss 0.141861\n",
      "iteration 1500 / 3000: loss 0.127726; validation loss 0.141530\n",
      "iteration 1600 / 3000: loss 0.126778; validation loss 0.141246\n",
      "iteration 1700 / 3000: loss 0.125899; validation loss 0.141000\n",
      "iteration 1800 / 3000: loss 0.125082; validation loss 0.140787\n",
      "iteration 1900 / 3000: loss 0.124318; validation loss 0.140602\n",
      "iteration 2000 / 3000: loss 0.123601; validation loss 0.140442\n",
      "iteration 2100 / 3000: loss 0.122927; validation loss 0.140302\n",
      "iteration 2200 / 3000: loss 0.122290; validation loss 0.140181\n",
      "iteration 2300 / 3000: loss 0.121688; validation loss 0.140076\n",
      "iteration 2400 / 3000: loss 0.121116; validation loss 0.139985\n",
      "iteration 2500 / 3000: loss 0.120573; validation loss 0.139906\n",
      "iteration 2600 / 3000: loss 0.120055; validation loss 0.139839\n",
      "iteration 2700 / 3000: loss 0.119560; validation loss 0.139781\n",
      "iteration 2800 / 3000: loss 0.119087; validation loss 0.139733\n",
      "iteration 2900 / 3000: loss 0.118634; validation loss 0.139691\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_splits = [list(train_and_val(X, y)) for i in range(5)]\n",
    "classifiers = []\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"Softmax Classifier {}\".format(i + 1))\n",
    "    \n",
    "    data_splits[i][0] = data_splits[i][0].astype(\"float64\")\n",
    "    data_splits[i][2] = data_splits[i][2].astype(\"float64\")\n",
    "    \n",
    "    data_splits[i][0] = np.hstack([data_splits[i][0], np.ones((data_splits[i][0].shape[0], 1))])\n",
    "    data_splits[i][2] = np.hstack([data_splits[i][2], np.ones((data_splits[i][2].shape[0], 1))])\n",
    "    \n",
    "    softmax = SoftmaxClassifier()\n",
    "    softmax.train(data_splits[i][0], data_splits[i][1],\n",
    "                  data_splits[i][2], data_splits[i][3],\n",
    "                  learning_rate=12, reg=0,\n",
    "                  num_iters=3000, verbose=True)\n",
    "    \n",
    "    classifiers.append(softmax)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.20823714938842042, 0.19764514905556788, 0.19173392207750345, 0.193521739564605, 0.19142561883711307]\n"
     ]
    }
   ],
   "source": [
    "### Predictions\n",
    "predictions = []\n",
    "scores = []\n",
    "\n",
    "for i in range(5):\n",
    "    y_test_pred = classifiers[i].predict(X_test)\n",
    "    y_val_pred = classifiers[i].predict(data_splits[i][2])\n",
    "    predictions.append(y_test_pred)\n",
    "    scores.append(rmse(y_val_pred, data_splits[i][3]))\n",
    "    \n",
    "# predictions = np.array(predictions)\n",
    "predictions = np.mean(predictions, axis=0)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for i, filename in enumerate(os.listdir(\"test\")):\n",
    "    data.append([int(filename[4:-4]), predictions[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns=['id', 'res'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>res</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>329</td>\n",
       "      <td>0.999968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2818</td>\n",
       "      <td>0.002383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>473</td>\n",
       "      <td>0.080105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1493</td>\n",
       "      <td>0.001379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>315</td>\n",
       "      <td>0.978596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id       res\n",
       "0   329  0.999968\n",
       "1  2818  0.002383\n",
       "2   473  0.080105\n",
       "3  1493  0.001379\n",
       "4   315  0.978596"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('submission.csv', columns=['id', 'res'], sep=',', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
